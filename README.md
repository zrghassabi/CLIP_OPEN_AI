useful Link:

https://medium.com/@Mert.A/zero-shot-image-classification-with-clip-and-huggingface-c73c438fce6d#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjczZTI1Zjk3ODkxMTljNzg3NWQ1ODA4N2E3OGFjMjNmNWVmMmVkYTMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDIzODU2OTYyMDA4MDg1ODEzNDEiLCJlbWFpbCI6Inouci5naGFzc2FiaUBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNzI5Nzk4OTg5LCJuYW1lIjoiUm96aXRhIEdoYXNzYWJpIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0s1dzdBTktySk00VlJNTTdGWVZfQVBBcXNnY0EzcWQzaG1qNDhJYW1EelVqMXhiN0huPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IlJveml0YSIsImZhbWlseV9uYW1lIjoiR2hhc3NhYmkiLCJpYXQiOjE3Mjk3OTkyODksImV4cCI6MTcyOTgwMjg4OSwianRpIjoiNDcwMTMxZTEwZDI5MzI0ZmYxMjk4YzExNzc0NjI2MGY3YzYyZGI5MCJ9.m6RWW_KN4BKWH7HW4_OoZk1l8oIa31KJVdzIF5C7M0I44XP8VD9feQoEw6bzIFvIv6y1z0M387c4EmoT0V22rXDKhd3cBuLPtpEAL8KGO4elMvRtubpCSZqz1pzyDi_PDl4P8Pl8uentnYAaeqDFw5-M05g520emsaylU7qkXmWgysJV4Yw3KnVRpmmS8llsxC0CM07bCNlFtcGcwTUXLbVnBtzrklpkqvZGTyMB0OtCTnZe42j5d-xHQiY8MloGgAY07ugTs86SAU9nqZBqRVfPVljvEbfSJl-Bx4JDL-VZhdOCYysq2sNVom7kiTArKA0nEl5mylKofiGGJpmObA

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://learnopencv.com/clip-model/

https://github.com/OpenAI/CLIP?tab=readme-ov-file

https://huggingface.co/docs/transformers/model_doc/clip

https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf

CLIP (Contrastive Language-Image Pre-training): This innovative model is pre-trained on a large dataset of text and image pairs. By learning to match text descriptions to corresponding images, CLIP develops an inherent understanding of the connection between language and visual content.

CLIP Paper: https://arxiv.org/pdf/2111.15664

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://openai.com/index/clip/

https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf

https://learnopencv.com/clip-model/

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://huggingface.co/docs/transformers/model_doc/clip

https://openai.com/index/clip/

https://arxiv.org/pdf/2111.15664

https://github.com/OpenAI/CLIP

https://github.com/mlfoundations/open_clip/blob/main/docs/Interacting_with_open_clip.ipynb


------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=GLa7z5rkSf4&t=73s

https://www.kaggle.com/code/moeinshariatnia/openai-clip-simple-implementation/notebook

MEDCLIP:

https://github.com/RyanWangZf/MedCLIP/tree/main

https://github.com/stanfordmlgroup/chexpert-labeler

CLIPSEG

https://huggingface.co/docs/transformers/model_doc/clipseg

https://github.com/timojl/clipseg/tree/master

visual prompt Tuning
https://github.com/KMnP/vpt




