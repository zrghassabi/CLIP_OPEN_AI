useful Link:

https://medium.com/@Mert.A/zero-shot-image-classification-with-clip-and-huggingface-c73c438fce6d#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjczZTI1Zjk3ODkxMTljNzg3NWQ1ODA4N2E3OGFjMjNmNWVmMmVkYTMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDIzODU2OTYyMDA4MDg1ODEzNDEiLCJlbWFpbCI6Inouci5naGFzc2FiaUBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNzI5Nzk4OTg5LCJuYW1lIjoiUm96aXRhIEdoYXNzYWJpIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0s1dzdBTktySk00VlJNTTdGWVZfQVBBcXNnY0EzcWQzaG1qNDhJYW1EelVqMXhiN0huPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IlJveml0YSIsImZhbWlseV9uYW1lIjoiR2hhc3NhYmkiLCJpYXQiOjE3Mjk3OTkyODksImV4cCI6MTcyOTgwMjg4OSwianRpIjoiNDcwMTMxZTEwZDI5MzI0ZmYxMjk4YzExNzc0NjI2MGY3YzYyZGI5MCJ9.m6RWW_KN4BKWH7HW4_OoZk1l8oIa31KJVdzIF5C7M0I44XP8VD9feQoEw6bzIFvIv6y1z0M387c4EmoT0V22rXDKhd3cBuLPtpEAL8KGO4elMvRtubpCSZqz1pzyDi_PDl4P8Pl8uentnYAaeqDFw5-M05g520emsaylU7qkXmWgysJV4Yw3KnVRpmmS8llsxC0CM07bCNlFtcGcwTUXLbVnBtzrklpkqvZGTyMB0OtCTnZe42j5d-xHQiY8MloGgAY07ugTs86SAU9nqZBqRVfPVljvEbfSJl-Bx4JDL-VZhdOCYysq2sNVom7kiTArKA0nEl5mylKofiGGJpmObA

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://learnopencv.com/clip-model/

https://github.com/OpenAI/CLIP?tab=readme-ov-file

https://huggingface.co/docs/transformers/model_doc/clip

https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf

CLIP (Contrastive Language-Image Pre-training): This innovative model is pre-trained on a large dataset of text and image pairs. By learning to match text descriptions to corresponding images, CLIP develops an inherent understanding of the connection between language and visual content.

CLIP Paper: https://arxiv.org/pdf/2111.15664

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://openai.com/index/clip/

https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf

https://learnopencv.com/clip-model/

https://www.geeksforgeeks.org/understanding-clip-vit-h-14safetensors-a-deep-dive-into-clip-models-and-safetensors/

https://huggingface.co/docs/transformers/model_doc/clip

https://openai.com/index/clip/

https://arxiv.org/pdf/2111.15664

https://github.com/OpenAI/CLIP

https://github.com/mlfoundations/open_clip/blob/main/docs/Interacting_with_open_clip.ipynb


------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=GLa7z5rkSf4&t=73s

https://www.kaggle.com/code/moeinshariatnia/openai-clip-simple-implementation/notebook

MEDCLIP:

https://github.com/RyanWangZf/MedCLIP/tree/main

https://github.com/stanfordmlgroup/chexpert-labeler

CLIPSEG

https://huggingface.co/docs/transformers/model_doc/clipseg

https://github.com/timojl/clipseg/tree/master

visual prompt Tuning
https://github.com/KMnP/vpt

DETR

https://github.com/KMnP/vpt

https://www.youtube.com/watch?v=T35ba_VXkMY&t=1693s

videoCLIP

https://www.youtube.com/watch?v=5gmsIeBeIRo

ActionCLIP

https://arxiv.org/pdf/2109.08472

https://github.com/sallymmx/ActionCLIP/blob/master/ActionCLIP.jpg

XCLIP

https://github.com/microsoft/VideoX/tree/master/X-CLIP

https://arxiv.org/pdf/2208.02816

S3D text-video instructional videos

https://github.com/antoine77340/S3D_HowTo100M

fairseg

https://github.com/facebookresearch/fairseq/tree/main

Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.

Open-clip

https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb

Open-COCO

https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb

Open-clip in hugging face

https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224

https://huggingface.co/models?library=open_clip

Laion

https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K

https://laion.ai/blog/laion-5b/

https://arxiv.org/pdf/2111.02114

CLIP retrieval 


https://github.com/rom1504/clip-retrieval

https://github.com/rom1504/clip-retrieval/blob/main/notebook/retrieval_example.ipynb

https://colab.research.google.com/github/criteo/autofaiss/blob/master/docs/notebooks/autofaiss_multimodal_search.ipynb#scrollTo=0seHT2GhnEC1

https://laion.ai/blog/laion-5b/

Fine tuning clip with sattelite images:

https://huggingface.co/blog/fine-tune-clip-rsicd

Kaggle

https://www.kaggle.com/code/moeinshariatnia/openai-clip-simple-implementation/notebook





